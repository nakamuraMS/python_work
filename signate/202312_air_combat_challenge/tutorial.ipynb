{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 応募用ファイルの作成手順"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは環境構築から始めて, エージェントを作成して投稿ができるフォーマットに整理し, 実際に対戦を実行して最終的に応募用ファイルを作成するまでの流れの一例を説明する."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 事前準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "google colaboratory上で環境構築をして実行を試みる. これによりGPU上でシミュレーションなどの実行が可能となる. まずこのファイル(`tutorial.ipynb`)を自身のドライブ(事前にgoogleアカウントを作成しておく必要がある)に保存しておき, google colaboratory上で開いておく. `seminar.zip`と配布シミュレータ`simulator_dist.zip`を`/content`直下にアップロードして, 解凍する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# osの確認\n",
    "!cat /etc/os-release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip simulator_dist.zip\n",
    "!unzip seminar.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`/content`直下に`simulator_dist`と`seminar`が存在していることを確認."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 環境構築"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ローカル環境で構築する場合と同様にC++依存パッケージやその他必要なPythonライブラリをインストールしてからシミュレータ本体をインストールする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### C++依存パッケージなどのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get -y install libboost-dev libnlopt-cxx-dev freeglut3-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /usr/local\n",
    "!wget -nc https://gitlab.com/libeigen/eigen/-/archive/3.4.0/eigen-3.4.0.zip\n",
    "!unzip eigen-3.4.0.zip\n",
    "!rm eigen-3.4.0.zip\n",
    "%cd eigen-3.4.0\n",
    "!mkdir build\n",
    "%cd build\n",
    "!cmake ..\n",
    "!make\n",
    "!make install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /usr/local/include\n",
    "!mkdir pybind11_json\n",
    "!cp /content/simulator_dist/root/thirdParty/include/pybind11_json/pybind11_json.hpp pybind11_json/pybind11_json.hpp\n",
    "!mkdir magic_enum\n",
    "!wget -O magic_enum/magic_enum.hpp https://github.com/Neargye/magic_enum/releases/download/v0.9.3/magic_enum.hpp\n",
    "!mkdir thread-pool\n",
    "!wget -O thread-pool/BS_thread_pool.hpp https://raw.githubusercontent.com/bshoshany/thread-pool/v3.3.0/BS_thread_pool.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nlohmann/json.git -b v3.11.2 --depth 1\n",
    "!cp -r json/include/nlohmann .\n",
    "!rm -r json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod 644 pybind11_json/pybind11_json.hpp magic_enum/magic_enum.hpp thread-pool/BS_thread_pool.hpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install timeout-decorator==0.5.0 keras-rl==0.4.2 pfrl==0.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonのバージョンは推奨バージョン(3.8)とは違うことに注意(現時点では3.10.12).\n",
    "\n",
    "また, Pytorchの推奨バージョンは`1.13.1`なので, 厳密に合わせたい場合は改めてインストールしなおしておく.\n",
    "\n",
    "```bash\n",
    "!pip install torch==1.13.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### シミュレータのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd /usr/local/src\n",
    "!cp -r /content/simulator_dist/root ASRCAISim1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd /usr/local/src/ASRCAISim1\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd /usr/local/src/ASRCAISim1/sample/modules/OriginalModelSample\n",
    "!pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10分以上かかる."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 実行テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "実際に正常に動くか確認する. `readme.md`にあるようにサンプルコードを実行する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%cd /content/simulator_dist/root/sample/scripts/R5Contest/MinimumEvaluation/\n",
    "!python evaluator.py \"Rule-Fixed\" \"Rule-Fixed\" -n 1 -l \"./results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " なお, GUIで再生はできない模様."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### その他"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dockerで構築する場合は容量などに余裕があればCPU版とGPU版を両方構築しておいて, 強化学習したい場合はGPU版を, 投稿テストをしたい場合はCPU版を使用するなどの方法がある.\n",
    "\n",
    "composeファイルを以下のように編集し,\n",
    "\n",
    "```yaml\n",
    "version: \"3\"\n",
    "services:\n",
    "  dev1:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.cpu\n",
    "    container_name: atla_cpu_gui\n",
    "    ports:\n",
    "      - \"8081:8080\"\n",
    "    volumes:\n",
    "      - .:/workspace\n",
    "    tty: true\n",
    "    user: root\n",
    "  dev2:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.gpu\n",
    "    deploy:\n",
    "      resources:\n",
    "        reservations:\n",
    "          devices:\n",
    "            - driver: nvidia\n",
    "              count: 1\n",
    "              capabilities: [gpu]\n",
    "    container_name: atla_gpu_gui\n",
    "    ports:\n",
    "      - \"8082:8080\"\n",
    "    volumes:\n",
    "      - .:/workspace\n",
    "    tty: true\n",
    "    user: root\n",
    "```\n",
    "\n",
    "docker composeコマンドを実行する.\n",
    "\n",
    "```bash\n",
    "docker compose up -d\n",
    "```\n",
    "\n",
    "すると, CPU版のコンテナ`atla_cpu_gui`とGPU版のコンテナ`atla_gpu_gui`が構築される. 後は任意の方法(`docker exec`やVSCodeの拡張機能などの利用)でコンテナに入って作業を行う."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## エージェントの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的には`simulator_dist/make_agent.ipynb`などに従うが, observationを作成する`make_obs`メソッドやその型(観測空間)を定義する`observation_space`メソッド, 行動空間を定義する`action_space`メソッドの作成の仕方についていくつか注意点がある."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### `make_obs`メソッドと`observation_space`メソッドの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actionを生成するためのobservationを作成する`make_obs`メソッドを実装する.\n",
    "\n",
    "- 強化学習を実装したい場合は基本的には`make_obs`メソッドで返る値は`numpy.ndarray`とすればよい(policyモデルに渡す想定).\n",
    "  - `observation_space`メソッドで`shape`を定義する必要がある. あらかじめどのようなobservationを作成するかを設計したうえで定義する.\n",
    "  - 対戦ログとしてobservationを取得するにはjson形式で保存できることが前提であるが, 今回の場合は`numpy.ndarray`は自動的に`list`に変換される.\n",
    "  - その他の型を返す場合はデフォルトでjson形式で保存できるような処理を挟む必要がある(`observation_space`メソッドとの整合も考慮).\n",
    "  - 特にログが欲しくない場合はそのままでよい(保存に失敗した場合, ログは勝敗結果と自分の陣営の色のみの情報になるが対戦結果自体はリーダーボードに反映される).\n",
    "- 行動判断モデルとして特に機械学習モデルを使用しないアルゴリズムを実装している場合は保存しておきたい情報をjson形式で保存できるように処理したものを返せばよい.\n",
    "  - そもそもログが必要ないなら適当に`0`などの値を返しておく.\n",
    "\n",
    "観測可能な主な情報は以下の通り.\n",
    "\n",
    "1. 自分と味方の機体諸元(位置, 速度, 姿勢, 角速度, 残弾数)\n",
    "1. 自分と味方の誘導弾諸元(位置, 速度, 目標ID, 誘導状態)\n",
    "1. 相手の機体諸元(位置と速度のみ)\n",
    "1. 相手の誘導弾諸元(方向のみ)\n",
    "1. 戦闘開始からの経過時間\n",
    "\n",
    "詳細は`docs/基準モデル及びパラメータに関する資料.pdf`の1.4.6項を参照されたい. 観測情報は階層構造になっていることに注目して例えば彼機の航跡は以下のような形で取得する.\n",
    "\n",
    "```Python\n",
    "parent.observables.at_p(\"/sensor/track\")\n",
    "```\n",
    "\n",
    "以下では例として数値ベクトル(自機の位置ベクトルと速度ベクトル, 検知している彼機の位置ベクトルと速度ベクトル, 自機の誘導弾の位置ベクトルと速度ベクトル, 残誘導弾の割合とmwsで検知している2次元航跡データを単純に並べてつなげた1次元の配列)として生成する. observationはステップごとにログとして残すことが可能であるが, json形式で保存できるフォーマットである必要があるが, 前述の通り`numpy.ndarray`として定義している場合は特に気にすることはない. それ以外の場合はあらかじめPythonプリミティブの型として作成するフラグ(ここでは`to_list`とする. 例えばconfigファイルで定義しておいて, インスタンス化したときに定義できるようにしておく)を定義しておいて, 適宜切り替えができるようにしておくとよい.\n",
    "\n",
    "`OriginalModelSample`モジュールでは単純な数値ベクトルだけでなく画像データとして生成するなど, より細かい作成方法が実装されているので, `R5PyAgentSample01S.py`や`R5PyAgentSample01M.py`などを参照されたい."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def makeObs(self):\n",
    "    # 味方機(自機含む)\n",
    "    self.ourMotion=[]\n",
    "    self.ourObservables=[]\n",
    "\n",
    "    for pIdx,parent in enumerate(self.parents.values()):\n",
    "        if(parent.isAlive()):\n",
    "            firstAlive=parent\n",
    "            break\n",
    "    for pIdx,parent in enumerate(self.parents.values()):\n",
    "        if(parent.isAlive()):\n",
    "            #残存していればobservablesそのもの\n",
    "            self.ourMotion.append(parent.observables[\"motion\"]())\n",
    "            self.ourObservables.append(parent.observables)\n",
    "            myMotion=MotionState(parent.observables[\"motion\"])\n",
    "\n",
    "        else:\n",
    "            self.ourMotion.append({})\n",
    "            #被撃墜or墜落済なら本体の更新は止まっているので残存している親が代理更新したものを取得(誘導弾情報のため)\n",
    "            self.ourObservables.append(\n",
    "                firstAlive.observables.at_p(\"/shared/fighter\").at(parent.getFullName()))\n",
    "\n",
    "    # 彼機(味方の誰かが探知しているもののみ)\n",
    "    # 観測されている航跡を、自陣営の機体に近いものから順にソートしてlastTrackInfoに格納する。\n",
    "    # lastTrackInfoは行動のdeployでも射撃対象の指定のために参照する。\n",
    "    def distance(track):\n",
    "        ret=-1.0\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(parent.isAlive()):\n",
    "                myMotion=MotionState(parent.observables[\"motion\"])\n",
    "                tmp=np.linalg.norm(track.posI()-myMotion.pos)\n",
    "                if(ret<0 or tmp<ret):\n",
    "                    ret=tmp\n",
    "        return ret\n",
    "    for pIdx,parent in enumerate(self.parents.values()):\n",
    "        if(parent.isAlive()):\n",
    "            self.lastTrackInfo=sorted([Track3D(t) for t in parent.observables.at_p(\"/sensor/track\")],key=distance) # type: ignore\n",
    "            break\n",
    "\n",
    "    # 味方誘導弾(射撃時刻が古いものから最大N発分)\n",
    "    # 味方の誘導弾を射撃時刻の古い順にソート\n",
    "    def launchedT(m):\n",
    "        return m[\"launchedT\"]() if m[\"isAlive\"]() and m[\"hasLaunched\"]() else np.inf\n",
    "    self.msls=sorted(sum([[m for m in f.at_p(\"/weapon/missiles\")] for f in self.ourObservables],[]),key=launchedT)\n",
    "\n",
    "    f_vec = [0.0]*6\n",
    "    for fIdx in range(len(self.ourMotion)):\n",
    "        if(fIdx>=self.maxTrackNum[\"Friend\"]):\n",
    "            break\n",
    "\n",
    "        if(self.ourObservables[fIdx][\"isAlive\"]()):\n",
    "            #初期弾数\n",
    "            numMsls=self.ourObservables[fIdx].at_p(\"/spec/weapon/numMsls\")()\n",
    "            #残弾数\n",
    "            remMsls=self.ourObservables[fIdx].at_p(\"/weapon/remMsls\")()\n",
    "\n",
    "            f_vec[3*fIdx] = remMsls/numMsls\n",
    "            #MWS検出情報\n",
    "            def angle(track):\n",
    "                return -np.dot(track.dirI(),myMotion.relBtoP(np.array([1,0,0])))\n",
    "            mws=sorted([Track2D(t) for t in self.ourObservables[fIdx].at_p(\"/sensor/mws/track\")],key=angle)\n",
    "            if len(mws):\n",
    "                f_vec[3*fIdx+1] = mws[0].dirI()[0]\n",
    "                f_vec[3*fIdx+2] = mws[0].dirI()[1]\n",
    "\n",
    "    # observationの作成\n",
    "    om_vec = []\n",
    "    for om in self.ourMotion:\n",
    "        if len(om)==0:\n",
    "            om_vec += [0.0]*6\n",
    "        else:\n",
    "            om_vec += om['pos']+om['vel']\n",
    "\n",
    "    lt_vec = []\n",
    "    for lt in self.lastTrackInfo:\n",
    "        lt_vec += list(lt.pos)+list(lt.vel)\n",
    "    n_lt = len(lt_vec)\n",
    "    if n_lt < 12:\n",
    "        lt_vec += [0.0]*(12-n_lt)\n",
    "\n",
    "    msls = [m() for m in self.msls]\n",
    "    m_vec = []\n",
    "    for msl in msls:\n",
    "        if msl['isAlive']:\n",
    "            m_vec += msl['motion']['pos']+msl['motion']['vel']\n",
    "        else:\n",
    "            m_vec += [0.0]*6\n",
    "\n",
    "    vec = om_vec + lt_vec + m_vec + f_vec\n",
    "\n",
    "    return np.array(vec, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "observationの形を定義する`observation_space`メソッドを実装する. observationは78次元の実数値ベクトルとなるのでそのように定義する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "\n",
    "def observation_space(self):\n",
    "    return spaces.Box(low=-np.inf,high=np.inf,shape=(78,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### `action_space`メソッドの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "actionは`deploy`メソッドに渡されて自機の動きや誘導弾発射の意思決定などを行うためのもの.\n",
    "\n",
    "以下の項目に関連するような行動を定義しておく.\n",
    "\n",
    "1. 自分と味方の機動（以下のいずれかによる）\n",
    "    - ロール、ピッチ、ヨー、スロットルの直接出力\n",
    "    - 進みたい方向と進みたい速度による抽象的な出力\n",
    "1. 射撃有無と射撃対象\n",
    "\n",
    "actionが存在すべき行動空間(このあと実装するpolicyの出力するactionの種類数)を定義する. ここではインスタンス化したときに各種設定(`self.actionDims`などの形でインスタンス化したときにshapeを定義しておく)をして(`OriginalModelSample/R5PyAgentSample01M.py`からの抜粋)10種類の離散値とする.\n",
    "\n",
    "なお, 強化学習を前提としない場合は特に気にすることはなく何らかの適当な値を返すような実装としておいてもよい(`spaces.Discrete()`を返すなど)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import spaces\n",
    "\n",
    "def action_space(self):\n",
    "    return spaces.MultiDiscrete(self.actionDims) # 行動空間の定義"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は`deploy`メソッドの実装例. policyが返した`action`をどのように渡しているか確認. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy(self,action):\n",
    "    self.last_action_obs=np.zeros([self.maxTrackNum[\"Friend\"],self.last_action_dim],dtype=np.float32)\n",
    "    action_idx=0\n",
    "\n",
    "    for pIdx,parent in enumerate(self.parents.values()):\n",
    "        if(not parent.isAlive()):\n",
    "            continue\n",
    "        actionInfo=self.actionInfos[pIdx]\n",
    "        myMotion=MotionState(parent.observables[\"motion\"])\n",
    "        #左右旋回\n",
    "        deltaAz=self.turnTable[action[action_idx]]\n",
    "        def angle(track):\n",
    "            return -np.dot(track.dirI(),myMotion.relBtoP(np.array([1,0,0])))\n",
    "        mws=sorted([Track2D(t) for t in parent.observables.at_p(\"/sensor/mws/track\")],key=angle)\n",
    "        if(len(mws)>0 and self.use_override_evasion):\n",
    "            deltaAz=self.evasion_turnTable[action[action_idx]]\n",
    "            dr=np.zeros([3])\n",
    "            for m in mws:\n",
    "                dr+=m.dirI()\n",
    "            dr/=np.linalg.norm(dr)\n",
    "            dstAz=atan2(-dr[1],-dr[0])+deltaAz\n",
    "            actionInfo.dstDir=np.array([cos(dstAz),sin(dstAz),0])\n",
    "        elif(self.dstAz_relative):\n",
    "            actionInfo.dstDir=myMotion.relHtoP(np.array([cos(deltaAz),sin(deltaAz),0]))\n",
    "        else:\n",
    "            actionInfo.dstDir=self.teamOrigin.relBtoP(np.array([cos(deltaAz),sin(deltaAz),0]))\n",
    "        action_idx+=1\n",
    "        dstAz=atan2(actionInfo.dstDir[1],actionInfo.dstDir[0])\n",
    "        self.last_action_obs[pIdx,0]=dstAz\n",
    "\n",
    "        #上昇・下降\n",
    "        if(self.use_altitude_command):\n",
    "            refAlt=round(-myMotion.pos(2)/self.refAltInterval)*self.refAltInterval\n",
    "            actionInfo.dstAlt=max(self.altMin,min(self.altMax,refAlt+self.altTable[action[action_idx]]))\n",
    "            dstPitch=0#dstAltをcommandsに与えればSixDoFFighter::FlightControllerのaltitudeKeeperで別途計算されるので0でよい。\n",
    "        else:\n",
    "            dstPitch=self.pitchTable[action[action_idx]]\n",
    "        action_idx+=1\n",
    "        actionInfo.dstDir=np.array([actionInfo.dstDir[0]*cos(dstPitch),actionInfo.dstDir[1]*cos(dstPitch),-sin(dstPitch)])\n",
    "        self.last_action_obs[pIdx,1]=actionInfo.dstAlt if self.use_altitude_command else dstPitch\n",
    "\n",
    "        #加減速\n",
    "        V=np.linalg.norm(myMotion.vel)\n",
    "        if(self.always_maxAB):\n",
    "            actionInfo.asThrottle=True\n",
    "            actionInfo.keepVel=False\n",
    "            actionInfo.dstThrottle=1.0\n",
    "            self.last_action_obs[pIdx,2]=1.0\n",
    "        else:\n",
    "            actionInfo.asThrottle=False\n",
    "            accel=self.accelTable[action[action_idx]]\n",
    "            action_idx+=1\n",
    "            actionInfo.dstV=V+accel # type: ignore\n",
    "            actionInfo.keepVel = accel==0.0\n",
    "            self.last_action_obs[pIdx,2]=accel/max(self.accelTable[-1],self.accelTable[0])\n",
    "        #下限速度の制限\n",
    "        if(V<self.minimumV):\n",
    "            actionInfo.velRecovery=True\n",
    "        if(V>=self.minimumRecoveryV):\n",
    "            actionInfo.velRecovery=False\n",
    "        if(actionInfo.velRecovery):\n",
    "            actionInfo.dstV=self.minimumRecoveryDstV\n",
    "            actionInfo.asThrottle=False\n",
    "\n",
    "        #射撃\n",
    "        #actionのパース\n",
    "        shotTarget=action[action_idx]-1\n",
    "        action_idx+=1\n",
    "        if(self.use_Rmax_fire):\n",
    "            if(len(self.shotIntervalTable)>1):\n",
    "                shotInterval=self.shotIntervalTable[action[action_idx]]\n",
    "                action_idx+=1\n",
    "            else:\n",
    "                shotInterval=self.shotIntervalTable[0]\n",
    "            if(len(self.shotThresholdTable)>1):\n",
    "                shotThreshold=self.shotThresholdTable[action[action_idx]]\n",
    "                action_idx+=1\n",
    "            else:\n",
    "                shotThreshold=self.shotThresholdTable[0]\n",
    "        #射撃可否の判断、射撃コマンドの生成\n",
    "        flyingMsls=0\n",
    "        for msl in parent.observables.at_p(\"/weapon/missiles\"):\n",
    "            if(msl.at(\"isAlive\")() and msl.at(\"hasLaunched\")()):\n",
    "                flyingMsls+=1\n",
    "        if(\n",
    "                shotTarget>=0 and\n",
    "                shotTarget<len(self.lastTrackInfo) and\n",
    "                parent.isLaunchableAt(self.lastTrackInfo[shotTarget]) and\n",
    "                flyingMsls<self.maxSimulShot\n",
    "        ):\n",
    "                if(self.use_Rmax_fire):\n",
    "                rMin=np.inf\n",
    "                t=self.lastTrackInfo[shotTarget]\n",
    "                r=self.calcRNorm(parent,myMotion,t)\n",
    "                if(r<=shotThreshold):\n",
    "                    #射程の条件を満たしている\n",
    "                    if(not t.truth in actionInfo.lastShotTimes):\n",
    "                        actionInfo.lastShotTimes[t.truth]=0.0\n",
    "                    if(self.manager.getTime()-actionInfo.lastShotTimes[t.truth]>=shotInterval):\n",
    "                        #射撃間隔の条件を満たしている\n",
    "                        actionInfo.lastShotTimes[t.truth]=self.manager.getTime()\n",
    "                    else:\n",
    "                        #射撃間隔の条件を満たさない\n",
    "                        shotTarget=-1\n",
    "                else:\n",
    "                    #射程の条件を満たさない\n",
    "                    shotTarget=-1\n",
    "        else:\n",
    "            shotTarget=-1\n",
    "        self.last_action_obs[pIdx,3+(shotTarget+1)]=1\n",
    "        if(shotTarget>=0):\n",
    "            actionInfo.launchFlag=True\n",
    "            actionInfo.target=self.lastTrackInfo[shotTarget]\n",
    "        else:\n",
    "            actionInfo.launchFlag=False\n",
    "            actionInfo.target=Track3D()\n",
    "        \n",
    "        self.observables[parent.getFullName()][\"decision\"]={\n",
    "            \"Roll\":(\"Don't care\"),\n",
    "            \"Fire\":(actionInfo.launchFlag,actionInfo.target.to_json())\n",
    "        }\n",
    "        if(len(mws)>0 and self.use_override_evasion):\n",
    "            self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_NED\",dstAz)\n",
    "        else:\n",
    "            if(self.dstAz_relative):\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_BODY\",deltaAz)\n",
    "            else:\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_NED\",dstAz)\n",
    "        if(self.use_altitude_command):\n",
    "            self.observables[parent.getFullName()][\"decision\"][\"Vertical\"]=(\"Pos\",-actionInfo.dstAlt)\n",
    "        else:\n",
    "            self.observables[parent.getFullName()][\"decision\"][\"Vertical\"]=(\"El\",-dstPitch)\n",
    "        if(actionInfo.asThrottle):\n",
    "            self.observables[parent.getFullName()][\"decision\"][\"Throttle\"]=(\"Throttle\",actionInfo.dstThrottle)\n",
    "        else:\n",
    "            self.observables[parent.getFullName()][\"decision\"][\"Throttle\"]=(\"Vel\",actionInfo.dstV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 全てをまとめる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下では定義したactionなどに合わせて`deploy`や`control`メソッドなどを含めたすべてを示してある. `control`メソッドでは場外に出ないような制御をかけている. これを`MyAgent.py`として作成しておく(中央集権方式)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import spaces\n",
    "from math import cos, sin, atan2, sqrt\n",
    "from ASRCAISim1.libCore import Agent, MotionState, Track3D, Track2D, getValueFromJsonKRD, deg2rad, StaticCollisionAvoider2D, LinearSegment, AltitudeKeeper\n",
    "\n",
    "\n",
    "class MyAgent(Agent):\n",
    "\n",
    "    class TeamOrigin():\n",
    "        #陣営座標系(進行方向が+x方向となるようにz軸まわりに回転させ、防衛ライン中央が原点となるように平行移動させた座標系)を表すクラス。\n",
    "        #MotionStateを使用しても良いがクォータニオンを経由することで浮動小数点演算に起因する余分な誤差が生じるため、もし可能な限り対称性を求めるのであればこの例のように符号反転で済ませたほうが良い。\n",
    "        #ただし、機体運動等も含めると全ての状態量に対して厳密に対称なシミュレーションとはならないため、ある程度の誤差は生じる。\n",
    "        def __init__(self,isEastSider_,dLine):\n",
    "            self.isEastSider=isEastSider_\n",
    "            if(self.isEastSider):\n",
    "                self.pos=np.array([0.,dLine,0.])\n",
    "            else:\n",
    "                self.pos=np.array([0.,-dLine,0.])\n",
    "        def relBtoP(self,v):\n",
    "            #陣営座標系⇛慣性座標系\n",
    "            if(self.isEastSider):\n",
    "                return np.array([v[1],-v[0],v[2]])\n",
    "            else:\n",
    "                return np.array([-v[1],v[0],v[2]])\n",
    "        def relPtoB(self,v):\n",
    "            #慣性座標系⇛陣営座標系\n",
    "            if(self.isEastSider):\n",
    "                return np.array([-v[1],v[0],v[2]])\n",
    "            else:\n",
    "                return np.array([v[1],-v[0],v[2]])\n",
    "\n",
    "\n",
    "    class ActionInfo():\n",
    "        def __init__(self):\n",
    "            self.dstDir=np.array([1.0,0.0,0.0]) #目標進行方向\n",
    "            self.dstAlt=10000.0 #目標高度\n",
    "            self.velRecovery=False #下限速度制限からの回復中かどうか\n",
    "            self.asThrottle=False #加減速についてスロットルでコマンドを生成するかどうか\n",
    "            self.keepVel=False #加減速について等速(dstAccel=0)としてコマンドを生成するかどうか\n",
    "            self.dstThrottle=1.0 #目標スロットル\n",
    "            self.dstV=300.0 #目標速度\n",
    "            self.launchFlag=False #射撃するかどうか\n",
    "            self.target=Track3D() #射撃対象\n",
    "            self.lastShotTimes={} #各Trackに対する直前の射撃時刻\n",
    "\n",
    "\n",
    "    def __init__(self,modelConfig,instanceConfig):\n",
    "        super().__init__(modelConfig,instanceConfig) # 設定の読み込み\n",
    "        if(self.isDummy):\n",
    "            return # Factoryによるダミー生成のために空引数でのインスタンス化に対応させる\n",
    "        self.time_params = self.modelConfig['interval']()\n",
    "        self.to_list = self.modelConfig['to_list']()\n",
    "        self.own = self.getTeam()\n",
    "        self.maxTrackNum=getValueFromJsonKRD(self.modelConfig,\"maxTrackNum\",self.randomGen,{\"Friend\":4,\"Enemy\":4})\n",
    "        self.last_action_dim=3+(1+self.maxTrackNum[\"Enemy\"])\n",
    "        self.maxMissileNum=getValueFromJsonKRD(self.modelConfig,\"maxMissileNum\",self.randomGen,{\"Friend\":8,\"Enemy\":1})\n",
    "        # 場外制限に関する設定\n",
    "        self.dOutLimit=getValueFromJsonKRD(self.modelConfig,\"dOutLimit\",self.randomGen,5000.0)\n",
    "        self.dOutLimitThreshold=getValueFromJsonKRD(self.modelConfig,\"dOutLimitThreshold\",self.randomGen,10000.0)\n",
    "        self.dOutLimitStrength=getValueFromJsonKRD(self.modelConfig,\"dOutLimitStrength\",self.randomGen,2e-3)\n",
    "        \n",
    "        # 左右旋回に関する設定\n",
    "        self.turnTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"turnTable\",self.randomGen,\n",
    "\t\t\t[-90.0,-45.0,-20.0,-10.0,0.0,10.0,20.0,45.0,90.0])),dtype=np.float64)\n",
    "        self.turnTable*=deg2rad(1.0)\n",
    "        self.use_override_evasion=getValueFromJsonKRD(self.modelConfig,\"use_override_evasion\",self.randomGen,True)\n",
    "        if(self.use_override_evasion):\n",
    "            self.evasion_turnTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"evasion_turnTable\",self.randomGen,\n",
    "                    [-90.0,-45.0,-20.0,-10.0,0.0,10.0,20.0,45.0,90.0])),dtype=np.float64)\n",
    "            self.evasion_turnTable*=deg2rad(1.0)\n",
    "            assert(len(self.turnTable)==len(self.evasion_turnTable))\n",
    "        else:\n",
    "            self.evasion_turnTable=self.turnTable\n",
    "        self.dstAz_relative=getValueFromJsonKRD(self.modelConfig,\"dstAz_relative\",self.randomGen,False)\n",
    "\n",
    "\t\t# 上昇・下降に関する設定\n",
    "        self.use_altitude_command=getValueFromJsonKRD(self.modelConfig,\"use_altitude_command\",self.randomGen,False)\n",
    "        if(self.use_altitude_command):\n",
    "            self.altTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"altTable\",self.randomGen,\n",
    "\t\t\t\t[-8000.0,-4000.0,-2000.0,-1000.0,0.0,1000.0,2000.0,4000.0,8000.0])),dtype=np.float64)\n",
    "            self.refAltInterval=getValueFromJsonKRD(self.modelConfig,\"refAltInterval\",self.randomGen,1000.0)\n",
    "        else:\n",
    "            self.pitchTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"pitchTable\",self.randomGen,\n",
    "\t\t\t\t[-45.0,-20.0,-10.0,-5.0,0.0,5.0,10.0,20.0,45.0])),dtype=np.float64)\n",
    "            self.pitchTable*=deg2rad(1.0)\n",
    "            self.refAltInterval=1.0\n",
    "        \n",
    "        # 加減速に関する設定\n",
    "        self.accelTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"accelTable\",self.randomGen,[-2.0,0.0,2.0])),dtype=np.float64)\n",
    "        self.always_maxAB=getValueFromJsonKRD(self.modelConfig,\"always_maxAB\",self.randomGen,False)\n",
    "        \n",
    "        # 射撃に関する設定\n",
    "        self.use_Rmax_fire=getValueFromJsonKRD(self.modelConfig,\"use_Rmax_fire\",self.randomGen,False)\n",
    "        if(self.use_Rmax_fire):\n",
    "            self.shotIntervalTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"shotIntervalTable\",self.randomGen,\n",
    "\t\t\t\t[5.0,10.0,20.0,40.0,80.0])),dtype=np.float64)\n",
    "            self.shotThresholdTable=np.array(sorted(getValueFromJsonKRD(self.modelConfig,\"shotThresholdTable\",self.randomGen,\n",
    "\t\t\t\t[0.0,0.25,0.5,0.75,1.0])),dtype=np.float64)\n",
    "        #行動制限に関する設定\n",
    "        #  高度制限に関する設定\n",
    "        self.altMin=getValueFromJsonKRD(self.modelConfig,\"altMin\",self.randomGen,2000.0)\n",
    "        self.altMax=getValueFromJsonKRD(self.modelConfig,\"altMax\",self.randomGen,15000.0)\n",
    "        self.altitudeKeeper=AltitudeKeeper(modelConfig().get(\"altitudeKeeper\",{}))\n",
    "        # 同時射撃数の制限に関する設定\n",
    "        self.maxSimulShot=getValueFromJsonKRD(self.modelConfig,\"maxSimulShot\",self.randomGen,4)\n",
    "        # 下限速度の制限に関する設定\n",
    "        self.minimumV=getValueFromJsonKRD(self.modelConfig,\"minimumV\",self.randomGen,150.0)\n",
    "        self.minimumRecoveryV=getValueFromJsonKRD(self.modelConfig,\"minimumRecoveryV\",self.randomGen,180.0)\n",
    "        self.minimumRecoveryDstV=getValueFromJsonKRD(self.modelConfig,\"minimumRecoveryDstV\",self.randomGen,200.0)\n",
    "        nvec=[]\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            nvec.append(len(self.turnTable))\n",
    "            nvec.append(len(self.altTable) if self.use_altitude_command else len(self.pitchTable))\n",
    "            if(not self.always_maxAB):\n",
    "                nvec.append(len(self.accelTable))\n",
    "            nvec.append(self.maxTrackNum[\"Enemy\"]+1)\n",
    "            if(self.use_Rmax_fire):\n",
    "                if(len(self.shotIntervalTable)>1):\n",
    "                    nvec.append(len(self.shotIntervalTable))\n",
    "                if(len(self.shotThresholdTable)>1):\n",
    "                    nvec.append(len(self.shotThresholdTable))\n",
    "        self.totalActionDim=1\n",
    "        self.actionDims=np.zeros([len(nvec)], dtype=int)\n",
    "        for i in range(len(nvec)):\n",
    "            self.totalActionDim*=nvec[i]\n",
    "            self.actionDims[i]=nvec[i]\n",
    "\n",
    "        self.actionInfos=[self.ActionInfo() for _ in self.parents]\n",
    "\n",
    "\n",
    "    def observation_space(self):\n",
    "        return spaces.Box(low=-np.inf, high=np.inf, shape=(78,))\n",
    "\n",
    "\n",
    "    def action_space(self):\n",
    "        return spaces.MultiDiscrete(self.actionDims)\n",
    "\n",
    "\n",
    "    def validate(self):\n",
    "        #Rulerに関する情報の取得\n",
    "        rulerObs=self.manager.getRuler()().observables()\n",
    "        self.dOut=rulerObs[\"dOut\"] # 戦域中心から場外ラインまでの距離\n",
    "        self.dLine=rulerObs[\"dLine\"] # 戦域中心から防衛ラインまでの距離\n",
    "        self.teamOrigin=self.TeamOrigin(self.own==rulerObs[\"eastSider\"],self.dLine) # 陣営座標系変換クラス定義\n",
    "\n",
    "\n",
    "    def makeObs(self):\n",
    "        # 味方機(自機含む)\n",
    "        self.ourMotion=[]\n",
    "        self.ourObservables=[]\n",
    "\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(parent.isAlive()):\n",
    "                firstAlive=parent\n",
    "                break\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(parent.isAlive()):\n",
    "                #残存していればobservablesそのもの\n",
    "                self.ourMotion.append(parent.observables[\"motion\"]())\n",
    "                self.ourObservables.append(parent.observables)\n",
    "                myMotion=MotionState(parent.observables[\"motion\"])\n",
    "\n",
    "            else:\n",
    "                self.ourMotion.append({})\n",
    "                #被撃墜or墜落済なら本体の更新は止まっているので残存している親が代理更新したものを取得(誘導弾情報のため)\n",
    "                self.ourObservables.append(\n",
    "                    firstAlive.observables.at_p(\"/shared/fighter\").at(parent.getFullName()))\n",
    "\n",
    "        # 彼機(味方の誰かが探知しているもののみ)\n",
    "        # 観測されている航跡を、自陣営の機体に近いものから順にソートしてlastTrackInfoに格納する。\n",
    "        # lastTrackInfoは行動のdeployでも射撃対象の指定のために参照する。\n",
    "        def distance(track):\n",
    "            ret=-1.0\n",
    "            for pIdx,parent in enumerate(self.parents.values()):\n",
    "                if(parent.isAlive()):\n",
    "                    myMotion=MotionState(parent.observables[\"motion\"])\n",
    "                    tmp=np.linalg.norm(track.posI()-myMotion.pos)\n",
    "                    if(ret<0 or tmp<ret):\n",
    "                        ret=tmp\n",
    "            return ret\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(parent.isAlive()):\n",
    "                self.lastTrackInfo=sorted([Track3D(t) for t in parent.observables.at_p(\"/sensor/track\")],key=distance) # type: ignore\n",
    "                break\n",
    "\n",
    "        # 味方誘導弾(射撃時刻が古いものから最大N発分)\n",
    "        # 味方の誘導弾を射撃時刻の古い順にソート\n",
    "        def launchedT(m):\n",
    "            return m[\"launchedT\"]() if m[\"isAlive\"]() and m[\"hasLaunched\"]() else np.inf\n",
    "        self.msls=sorted(sum([[m for m in f.at_p(\"/weapon/missiles\")] for f in self.ourObservables],[]),key=launchedT)\n",
    "\n",
    "        f_vec = [0.0]*6\n",
    "        for fIdx in range(len(self.ourMotion)):\n",
    "            if(fIdx>=self.maxTrackNum[\"Friend\"]):\n",
    "                break\n",
    "\n",
    "            if(self.ourObservables[fIdx][\"isAlive\"]()):\n",
    "                #初期弾数\n",
    "                numMsls=self.ourObservables[fIdx].at_p(\"/spec/weapon/numMsls\")()\n",
    "                #残弾数\n",
    "                remMsls=self.ourObservables[fIdx].at_p(\"/weapon/remMsls\")()\n",
    "\n",
    "                f_vec[3*fIdx] = remMsls/numMsls\n",
    "                #MWS検出情報\n",
    "                def angle(track):\n",
    "                    return -np.dot(track.dirI(),myMotion.relBtoP(np.array([1,0,0])))\n",
    "                mws=sorted([Track2D(t) for t in self.ourObservables[fIdx].at_p(\"/sensor/mws/track\")],key=angle)\n",
    "                if len(mws):\n",
    "                    f_vec[3*fIdx+1] = mws[0].dirI()[0]\n",
    "                    f_vec[3*fIdx+2] = mws[0].dirI()[1]\n",
    "\n",
    "        # observationの作成\n",
    "        om_vec = []\n",
    "        for om in self.ourMotion:\n",
    "            if len(om)==0:\n",
    "                om_vec += [0.0]*6\n",
    "            else:\n",
    "                om_vec += om['pos']+om['vel']\n",
    "\n",
    "        lt_vec = []\n",
    "        for lt in self.lastTrackInfo:\n",
    "            lt_vec += list(lt.pos)+list(lt.vel)\n",
    "        n_lt = len(lt_vec)\n",
    "        if n_lt < 12:\n",
    "            lt_vec += [0.0]*(12-n_lt)\n",
    "\n",
    "        msls = [m() for m in self.msls]\n",
    "        m_vec = []\n",
    "        for msl in msls:\n",
    "            if msl['isAlive']:\n",
    "                m_vec += msl['motion']['pos']+msl['motion']['vel']\n",
    "            else:\n",
    "                m_vec += [0.0]*6\n",
    "\n",
    "        vec = om_vec + lt_vec + m_vec + f_vec\n",
    "\n",
    "        return np.array(vec, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def deploy(self,action):\n",
    "        self.last_action_obs=np.zeros([self.maxTrackNum[\"Friend\"],self.last_action_dim],dtype=np.float32)\n",
    "        action_idx=0\n",
    "\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(not parent.isAlive()):\n",
    "                continue\n",
    "            actionInfo=self.actionInfos[pIdx]\n",
    "            myMotion=MotionState(parent.observables[\"motion\"])\n",
    "\t\t\t#左右旋回\n",
    "            deltaAz=self.turnTable[action[action_idx]]\n",
    "            def angle(track):\n",
    "                return -np.dot(track.dirI(),myMotion.relBtoP(np.array([1,0,0])))\n",
    "            mws=sorted([Track2D(t) for t in parent.observables.at_p(\"/sensor/mws/track\")],key=angle)\n",
    "            if(len(mws)>0 and self.use_override_evasion):\n",
    "                deltaAz=self.evasion_turnTable[action[action_idx]]\n",
    "                dr=np.zeros([3])\n",
    "                for m in mws:\n",
    "                    dr+=m.dirI()\n",
    "                dr/=np.linalg.norm(dr)\n",
    "                dstAz=atan2(-dr[1],-dr[0])+deltaAz\n",
    "                actionInfo.dstDir=np.array([cos(dstAz),sin(dstAz),0])\n",
    "            elif(self.dstAz_relative):\n",
    "                actionInfo.dstDir=myMotion.relHtoP(np.array([cos(deltaAz),sin(deltaAz),0]))\n",
    "            else:\n",
    "                actionInfo.dstDir=self.teamOrigin.relBtoP(np.array([cos(deltaAz),sin(deltaAz),0]))\n",
    "            action_idx+=1\n",
    "            dstAz=atan2(actionInfo.dstDir[1],actionInfo.dstDir[0])\n",
    "            self.last_action_obs[pIdx,0]=dstAz\n",
    "\n",
    "\t\t\t#上昇・下降\n",
    "            if(self.use_altitude_command):\n",
    "                refAlt=round(-myMotion.pos(2)/self.refAltInterval)*self.refAltInterval\n",
    "                actionInfo.dstAlt=max(self.altMin,min(self.altMax,refAlt+self.altTable[action[action_idx]]))\n",
    "                dstPitch=0#dstAltをcommandsに与えればSixDoFFighter::FlightControllerのaltitudeKeeperで別途計算されるので0でよい。\n",
    "            else:\n",
    "                dstPitch=self.pitchTable[action[action_idx]]\n",
    "            action_idx+=1\n",
    "            actionInfo.dstDir=np.array([actionInfo.dstDir[0]*cos(dstPitch),actionInfo.dstDir[1]*cos(dstPitch),-sin(dstPitch)])\n",
    "            self.last_action_obs[pIdx,1]=actionInfo.dstAlt if self.use_altitude_command else dstPitch\n",
    "\n",
    "\t\t\t#加減速\n",
    "            V=np.linalg.norm(myMotion.vel)\n",
    "            if(self.always_maxAB):\n",
    "                actionInfo.asThrottle=True\n",
    "                actionInfo.keepVel=False\n",
    "                actionInfo.dstThrottle=1.0\n",
    "                self.last_action_obs[pIdx,2]=1.0\n",
    "            else:\n",
    "                actionInfo.asThrottle=False\n",
    "                accel=self.accelTable[action[action_idx]]\n",
    "                action_idx+=1\n",
    "                actionInfo.dstV=V+accel # type: ignore\n",
    "                actionInfo.keepVel = accel==0.0\n",
    "                self.last_action_obs[pIdx,2]=accel/max(self.accelTable[-1],self.accelTable[0])\n",
    "            #下限速度の制限\n",
    "            if(V<self.minimumV):\n",
    "                actionInfo.velRecovery=True\n",
    "            if(V>=self.minimumRecoveryV):\n",
    "                actionInfo.velRecovery=False\n",
    "            if(actionInfo.velRecovery):\n",
    "                actionInfo.dstV=self.minimumRecoveryDstV\n",
    "                actionInfo.asThrottle=False\n",
    "\n",
    "            #射撃\n",
    "            #actionのパース\n",
    "            shotTarget=action[action_idx]-1\n",
    "            action_idx+=1\n",
    "            if(self.use_Rmax_fire):\n",
    "                if(len(self.shotIntervalTable)>1):\n",
    "                    shotInterval=self.shotIntervalTable[action[action_idx]]\n",
    "                    action_idx+=1\n",
    "                else:\n",
    "                    shotInterval=self.shotIntervalTable[0]\n",
    "                if(len(self.shotThresholdTable)>1):\n",
    "                    shotThreshold=self.shotThresholdTable[action[action_idx]]\n",
    "                    action_idx+=1\n",
    "                else:\n",
    "                    shotThreshold=self.shotThresholdTable[0]\n",
    "            #射撃可否の判断、射撃コマンドの生成\n",
    "            flyingMsls=0\n",
    "            for msl in parent.observables.at_p(\"/weapon/missiles\"):\n",
    "                if(msl.at(\"isAlive\")() and msl.at(\"hasLaunched\")()):\n",
    "                    flyingMsls+=1\n",
    "            if(\n",
    "                 shotTarget>=0 and\n",
    "                 shotTarget<len(self.lastTrackInfo) and\n",
    "                 parent.isLaunchableAt(self.lastTrackInfo[shotTarget]) and\n",
    "                 flyingMsls<self.maxSimulShot\n",
    "            ):\n",
    "                 if(self.use_Rmax_fire):\n",
    "                    rMin=np.inf\n",
    "                    t=self.lastTrackInfo[shotTarget]\n",
    "                    r=self.calcRNorm(parent,myMotion,t)\n",
    "                    if(r<=shotThreshold):\n",
    "                        #射程の条件を満たしている\n",
    "                        if(not t.truth in actionInfo.lastShotTimes):\n",
    "                            actionInfo.lastShotTimes[t.truth]=0.0\n",
    "                        if(self.manager.getTime()-actionInfo.lastShotTimes[t.truth]>=shotInterval):\n",
    "                            #射撃間隔の条件を満たしている\n",
    "                            actionInfo.lastShotTimes[t.truth]=self.manager.getTime()\n",
    "                        else:\n",
    "                            #射撃間隔の条件を満たさない\n",
    "                            shotTarget=-1\n",
    "                    else:\n",
    "                        #射程の条件を満たさない\n",
    "                        shotTarget=-1\n",
    "            else:\n",
    "                shotTarget=-1\n",
    "            self.last_action_obs[pIdx,3+(shotTarget+1)]=1\n",
    "            if(shotTarget>=0):\n",
    "                actionInfo.launchFlag=True\n",
    "                actionInfo.target=self.lastTrackInfo[shotTarget]\n",
    "            else:\n",
    "                actionInfo.launchFlag=False\n",
    "                actionInfo.target=Track3D()\n",
    "            \n",
    "            self.observables[parent.getFullName()][\"decision\"]={\n",
    "                \"Roll\":(\"Don't care\"),\n",
    "                \"Fire\":(actionInfo.launchFlag,actionInfo.target.to_json())\n",
    "            }\n",
    "            if(len(mws)>0 and self.use_override_evasion):\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_NED\",dstAz)\n",
    "            else:\n",
    "                if(self.dstAz_relative):\n",
    "                    self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_BODY\",deltaAz)\n",
    "                else:\n",
    "                    self.observables[parent.getFullName()][\"decision\"][\"Horizontal\"]=(\"Az_NED\",dstAz)\n",
    "            if(self.use_altitude_command):\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Vertical\"]=(\"Pos\",-actionInfo.dstAlt)\n",
    "            else:\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Vertical\"]=(\"El\",-dstPitch)\n",
    "            if(actionInfo.asThrottle):\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Throttle\"]=(\"Throttle\",actionInfo.dstThrottle)\n",
    "            else:\n",
    "                self.observables[parent.getFullName()][\"decision\"][\"Throttle\"]=(\"Vel\",actionInfo.dstV)\n",
    "\n",
    "\n",
    "    def control(self):\n",
    "\t\t#Setup collision avoider\n",
    "        avoider=StaticCollisionAvoider2D()\n",
    "        #北側\n",
    "        c={\n",
    "            \"p1\":np.array([+self.dOut,-5*self.dLine,0]),\n",
    "            \"p2\":np.array([+self.dOut,+5*self.dLine,0]),\n",
    "            \"infinite_p1\":True,\n",
    "            \"infinite_p2\":True,\n",
    "            \"isOneSide\":True,\n",
    "            \"inner\":np.array([0.0,0.0]),\n",
    "            \"limit\":self.dOutLimit,\n",
    "            \"threshold\":self.dOutLimitThreshold,\n",
    "            \"adjustStrength\":self.dOutLimitStrength,\n",
    "        }\n",
    "        avoider.borders.append(LinearSegment(c))\n",
    "        #南側\n",
    "        c={\n",
    "            \"p1\":np.array([-self.dOut,-5*self.dLine,0]),\n",
    "            \"p2\":np.array([-self.dOut,+5*self.dLine,0]),\n",
    "            \"infinite_p1\":True,\n",
    "            \"infinite_p2\":True,\n",
    "            \"isOneSide\":True,\n",
    "            \"inner\":np.array([0.0,0.0]),\n",
    "            \"limit\":self.dOutLimit,\n",
    "            \"threshold\":self.dOutLimitThreshold,\n",
    "            \"adjustStrength\":self.dOutLimitStrength,\n",
    "        }\n",
    "        avoider.borders.append(LinearSegment(c))\n",
    "        #東側\n",
    "        c={\n",
    "            \"p1\":np.array([-5*self.dOut,+self.dLine,0]),\n",
    "            \"p2\":np.array([+5*self.dOut,+self.dLine,0]),\n",
    "            \"infinite_p1\":True,\n",
    "            \"infinite_p2\":True,\n",
    "            \"isOneSide\":True,\n",
    "            \"inner\":np.array([0.0,0.0]),\n",
    "            \"limit\":self.dOutLimit,\n",
    "            \"threshold\":self.dOutLimitThreshold,\n",
    "            \"adjustStrength\":self.dOutLimitStrength,\n",
    "        }\n",
    "        avoider.borders.append(LinearSegment(c))\n",
    "        #西側\n",
    "        c={\n",
    "            \"p1\":np.array([-5*self.dOut,-self.dLine,0]),\n",
    "            \"p2\":np.array([+5*self.dOut,-self.dLine,0]),\n",
    "            \"infinite_p1\":True,\n",
    "            \"infinite_p2\":True,\n",
    "            \"isOneSide\":True,\n",
    "            \"inner\":np.array([0.0,0.0]),\n",
    "            \"limit\":self.dOutLimit,\n",
    "            \"threshold\":self.dOutLimitThreshold,\n",
    "            \"adjustStrength\":self.dOutLimitStrength,\n",
    "        }\n",
    "        avoider.borders.append(LinearSegment(c))\n",
    "        for pIdx,parent in enumerate(self.parents.values()):\n",
    "            if(not parent.isAlive()):\n",
    "                continue\n",
    "            actionInfo=self.actionInfos[pIdx]\n",
    "            myMotion=MotionState(parent.observables[\"motion\"])\n",
    "            pos=myMotion.pos\n",
    "            vel=myMotion.vel\n",
    "            #戦域逸脱を避けるための方位補正\n",
    "            actionInfo.dstDir=avoider(myMotion,actionInfo.dstDir)\n",
    "            #高度方向の補正(actionがピッチ指定の場合)\n",
    "            if(not self.use_altitude_command):\n",
    "                n=sqrt(actionInfo.dstDir[0]*actionInfo.dstDir[0]+actionInfo.dstDir[1]*actionInfo.dstDir[1])\n",
    "                dstPitch=atan2(-actionInfo.dstDir[2],n)\n",
    "                #高度下限側\n",
    "                bottom=self.altitudeKeeper(myMotion,actionInfo.dstDir,self.altMin)\n",
    "                minPitch=atan2(-bottom[2],sqrt(bottom[0]*bottom[0]+bottom[1]*bottom[1]))\n",
    "                #高度上限側\n",
    "                top=self.altitudeKeeper(myMotion,actionInfo.dstDir,self.altMax)\n",
    "                maxPitch=atan2(-top[2],sqrt(top[0]*top[0]+top[1]*top[1]))\n",
    "                dstPitch=max(minPitch,min(maxPitch,dstPitch))\n",
    "                cs=cos(dstPitch)\n",
    "                sn=sin(dstPitch)\n",
    "                actionInfo.dstDir=np.array([actionInfo.dstDir[0]/n*cs,actionInfo.dstDir[1]/n*cs,-sn])\n",
    "            self.commands[parent.getFullName()]={\n",
    "                \"motion\":{\n",
    "                    \"dstDir\":actionInfo.dstDir\n",
    "                },\n",
    "                \"weapon\":{\n",
    "                    \"launch\":actionInfo.launchFlag,\n",
    "                    \"target\":actionInfo.target.to_json()\n",
    "                }\n",
    "            }\n",
    "            if(self.use_altitude_command):\n",
    "                self.commands[parent.getFullName()][\"motion\"][\"dstAlt\"]=actionInfo.dstAlt\n",
    "            if(actionInfo.asThrottle):\n",
    "                self.commands[parent.getFullName()][\"motion\"][\"dstThrottle\"]=actionInfo.dstThrottle\n",
    "            elif(actionInfo.keepVel):\n",
    "                self.commands[parent.getFullName()][\"motion\"][\"dstAccel\"]=0.0\n",
    "            else:\n",
    "                self.commands[parent.getFullName()][\"motion\"][\"dstV\"]=actionInfo.dstV\n",
    "            actionInfo.launchFlag=False\n",
    "\n",
    "    \n",
    "    def calcRHead(self,parent,myMotion,track):\n",
    "        #相手が現在の位置、速度で直ちに正面を向いて水平飛行になった場合の射程(RHead)を返す。\n",
    "        rt=track.posI()\n",
    "        vt=track.velI()\n",
    "        rs=myMotion.pos\n",
    "        vs=myMotion.vel\n",
    "        return parent.getRmax(rs,vs,rt,vt,np.pi)\n",
    "    \n",
    "    \n",
    "    def calcRTail(self,parent,myMotion,track):\n",
    "        #相手が現在の位置、速度で直ちに背後を向いて水平飛行になった場合の射程(RTail)を返す。\n",
    "        rt=track.posI()\n",
    "        vt=track.velI()\n",
    "        rs=myMotion.pos\n",
    "        vs=myMotion.vel\n",
    "        return parent.getRmax(rs,vs,rt,vt,0.0)\n",
    "\n",
    "    \n",
    "    def calcRNorm(self,parent,myMotion,track):\n",
    "        #RTail→0、RHead→1として正規化した距離を返す。\n",
    "        RHead=self.calcRHead(parent,myMotion,track)\n",
    "        RTail=self.calcRTail(parent,myMotion,track)\n",
    "        rs=myMotion.pos\n",
    "        rt=track.posI()\n",
    "        r=np.linalg.norm(rs-rt)-RTail\n",
    "        delta=RHead-RTail\n",
    "        outRangeScale=100000.0\n",
    "        if(delta==0):\n",
    "            if(r<0):\n",
    "                r=r/outRangeScale\n",
    "            elif(r>0):\n",
    "                r=1+r/outRangeScale\n",
    "            else:\n",
    "                r=0\n",
    "        else:\n",
    "            if(r<0):\n",
    "                r=r/outRangeScale\n",
    "            elif(r>delta):\n",
    "                r=1+(r-delta)/outRangeScale\n",
    "            else:\n",
    "                r/=delta\n",
    "        return r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## policyの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配布した`make_agent_rl/make_agent.ipynb`に従う.\n",
    "\n",
    "observationを渡してactionを返すpolicyモデルを実装する. ここでは`OriginalModelSample.R5TorchNNSampleForHandyRL.R5TorchNNSampleForHandyRL`により深層学習モデルを構築する前提とする. \n",
    "\n",
    "コンフィグファイル`sample_config.yml`の`policy_config`->`Learner`->`model_config`を適宜編集して深層学習モデルを新たに定義することができる.\n",
    "\n",
    "```Yaml\n",
    "dense:\n",
    "    layers:\n",
    "        - [\"Linear\",{\"out_features\": 128}]\n",
    "        - [\"ReLU\",{}]\n",
    "        - [\"ResidualBlock\",{\n",
    "            \"layers\":[\n",
    "                [\"Linear\",{\"out_features\": 128}],\n",
    "                [\"BatchNorm1d\",{}]\n",
    "            ]}]\n",
    "        - [\"ReLU\",{}]\n",
    "        - [\"ResidualBlock\",{\n",
    "            \"layers\":[\n",
    "                [\"Linear\",{\"out_features\": 128}],\n",
    "                [\"BatchNorm1d\",{}]\n",
    "            ]}]\n",
    "\n",
    "```\n",
    "\n",
    "`layers`の中で層を増やしたりノード数を増やしたりして深層学習モデルの構造を新たに定義する. 前に設定した`observation_space`や`action_space`によって入力層や出力層が決まる. \n",
    "\n",
    "対戦を実行する際は強化学習フレームワークから独立させてPolicyを使用するためのインターフェースによりpolicyを作成する(ここでは提供されている`ASRCAISim1.addons.HandyRLUtility.StandaloneHandyRLPolicy.StandaloneHandyRLPolicy`を使用). importして呼べるようにしておく.\n",
    "\n",
    "強化学習を前提としない場合は投稿プログラム内で適当な値を返すダミーpolicyを実装しておく."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, yaml\n",
    "from OriginalModelSample.R5TorchNNSampleForHandyRL import R5TorchNNSampleForHandyRL\n",
    "from ASRCAISim1.addons.HandyRLUtility.distribution import getActionDistributionClass\n",
    "from ASRCAISim1.addons.HandyRLUtility.StandaloneHandyRLPolicy import StandaloneHandyRLPolicy\n",
    "\n",
    "model_config=yaml.safe_load(open(os.path.join(os.path.dirname(__file__),\"model_config.yaml\"),\"r\"))\n",
    "weightPath = None\n",
    "isDeterministic=False #決定論的に行動させたい場合はTrue、確率論的に行動させたい場合はFalseとする\n",
    "policy = StandaloneHandyRLPolicy(R5TorchNNSampleForHandyRL,model_config,weightPath,getActionDistributionClass,isDeterministic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Factoryへの追加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "配布した`make_agent_rl/make_agent.ipynb`に従う."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 独自のAgentの登録"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習を実行するときに作成したエージェントを呼べるように登録しておく必要がある.\n",
    "\n",
    "`./sample_config.yml`の\"env_args\"->\"env\"の値で実際に環境構築を行うモジュールを選択している(デフォルトでは`sample`という名前になっていて, `handyrl.envs.SampleEnv.sample.py`が利用されることになる.)が, シミュレーション環境構築時(`handyrl.envs.SampleEnv.sample.py`のL56-76の部分)でエージェントの登録を行っている.\n",
    "\n",
    "```Python\n",
    "# エージェントの登録\n",
    "userModelID=args[\"userModelID\"]\n",
    "userModuleID=args[\"userModuleID\"]\n",
    "with open(os.path.join(userModuleID, args[\"modelargs\"])) as f:\n",
    "    model_args = json.load(f)\n",
    "\n",
    "module = importlib.import_module(userModuleID)\n",
    "assert hasattr(module, \"getUserAgentClass\")\n",
    "assert hasattr(module, \"getUserAgentModelConfig\")\n",
    "assert hasattr(module, \"isUserAgentSingleAsset\")\n",
    "assert hasattr(module, \"getUserPolicy\")\n",
    "\n",
    "agentClass = module.getUserAgentClass(model_args)\n",
    "addPythonClass(\"Agent\", \"Agent_\"+userModelID, agentClass)\n",
    "Factory.addDefaultModel(\n",
    "    \"Agent\",\n",
    "    \"Agent_\"+userModelID,\n",
    "    {\n",
    "        \"class\": \"Agent_\"+userModelID,\n",
    "        \"config\": module.getUserAgentModelConfig(model_args)\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`userModuleID`という名前のディレクトリに`__init__.py`があって, インポートしている. デフォルトでは`Test`という名前としている(`./Test`以下を実際に確認されたい.).\n",
    "\n",
    "`./sample_config.yml`の\"env_args\"の\"userModelID\"の値`userModelID`が`Agent_{userModelID}`という形でクラス名として登録される. この名前と`R5_contest_learning_config_{M,S}.json`の\"AgentConfigDispatcher\"に記載の\"Learner_e\"の\"model\"の値を一致させることで作成したエージェントによる学習が実行可能となる(デフォルトでは\"userModelID\"は`Sample`としているので`Agent_Sample`としている. 各自確認されたい.).\n",
    "\n",
    "ここでは特に扱っていないが, 並列実行する場合は全てのインスタンス上でFactoryへの追加が行われる必要がある(EnvironmentはWorkerごとにインスタンス化される(__init__がWorkerごとに呼ばれる)ため, 全てのインスタンスでFactoryへのモデル追加が独立に行われる).\n",
    "\n",
    "デフォルトでは中央集権型として扱うため`R5_contest_learning_config_M.json`を自作エージェントに合わせるように編集している.\n",
    "\n",
    "エージェント登録の別の方法として, OriginalModelSampleに独自のクラスを作成しておいて(ビルドして`site-packages`を更新)`configs/R5_contest_agent_ruler_reward_models.json`で\"Factory\"の\"Agent\"項目で好きな名前(`新エージェント名`とする)を登録して, その\"class\"において, 作成した独自のクラス名を記載しておくことも可能. その際は\"AgentConfigDispatcher\"に記載の\"Learner_e\"の\"model\"の値を`新エージェント名`にする.\n",
    "\n",
    "投稿可能なプログラムを作成するときはクラスを記述する部分を別のpyファイルとして作成してインポートできるようにしたり, `__init__.py`に直接書き込んでその中で呼べるようにしておく必要がある. 以下は`__init__.py`に直接書き込む場合の例.\n",
    "\n",
    "```Python\n",
    "def getUserAgentClass(args={}):\n",
    "    import 独自のクラス名\n",
    "    return 独自のクラス名\n",
    "\n",
    "class 独自のクラス名():\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 独自のRewardの登録\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "シミュレーション環境構築時(`handyrl.envs.SampleEnv.sample.py`のL79-95の部分)で報酬の登録を行っている.\n",
    "\n",
    "```Python\n",
    "# 報酬の登録\n",
    "userRewardModuleID = args[\"userRewardModuleID\"]\n",
    "with open(args[\"rewardConfig\"]) as f:\n",
    "    reward_config = json.load(f)\n",
    "\n",
    "reward_module = importlib.import_module(userRewardModuleID)\n",
    "\n",
    "rewardClass = reward_module.MyReward\n",
    "addPythonClass(\"Reward\", userRewardModuleID, rewardClass)\n",
    "Factory.addDefaultModel(\n",
    "    \"Reward\",\n",
    "    userRewardModuleID,\n",
    "    {\n",
    "        \"class\": \"MyReward\",\n",
    "        \"config\": reward_config\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "`seminar/MyReward.py`の`MyReward`モジュールを読み込んで`seminar/reward_config.json`で記述されている設定ファイルを渡して登録をしている. この実装ではクラス名は`MyReward`で固定する必要がある. `seminar/MyReward.py`では, `onInnerStepEnd`メソッドにおいてインナーステップ終了時に残存機数に応じて報酬を与えるシンプルなものとなっている. 適宜改修されたい. また, `docs/基準シミュレータ 取扱説明書.pdf`の5.3項も参照されたい. `OriginalModelSample`にも`R5PyRewardSample01.py`などに報酬の実装例があるため, こちらも参照すること.\n",
    "\n",
    "またエージェントの場合と同様に, OriginalModelSampleに独自のクラスを作成しておいて(ビルドして`site-packages`を更新)`configs/R5_contest_agent_ruler_reward_models.json`で\"Factory\"の\"Reward\"項目で好きな名前(`新報酬名`とする)を登録して, その\"class\"において, 作成した独自のクラス名を記載しておくことも可能.\n",
    "\n",
    "学習設定ファイル`configs/R5_contest_learning_config_{M/S}.json`の\"Reward\"で登録する報酬を以下のようにリストで渡すとその合計値を実際の報酬として返す. ここで実装している`seminar/MyReward.py`を報酬として学習させたい場合, `sample_config.yml`の\"env_args\"の\"userRewardModuleID\"の値を以下のように\"model\"の値として記述しておく. \"target\"を以下のように\"All\"とすると場に存在する者すべての陣営及びAgentが計算対象となる. 詳細は`docs/基準シミュレータ 取扱説明書.pdf`の表4.6-2などを参照.\n",
    "\n",
    "```json\n",
    "[\n",
    "    {\"model\":\"MyReward\",\"target\":\"All\"},\n",
    "    {\"model\":\"MyWinLoseReward\",\"target\":\"All\"}\n",
    "]\n",
    "```\n",
    "\n",
    "以下は`MyReward.py`の実装例."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ASRCAISim1.libCore import TeamReward, nljson, Fighter\n",
    "\n",
    "\n",
    "class MyReward(TeamReward):\n",
    "    \"\"\"\n",
    "    チーム全体で共有する報酬は TeamReward を継承し、\n",
    "    個別の Agent に与える報酬は AgentReward を継承する。\n",
    "    \"\"\"\n",
    "    def __init__(self, modelConfig: nljson, instanceConfig: nljson):\n",
    "        super().__init__(modelConfig, instanceConfig)\n",
    "        if(self.isDummy):\n",
    "            return #Factory によるダミー生成のために空引数でのインスタンス化に対応させる\n",
    "\n",
    "\n",
    "    def onEpisodeBegin(self):\n",
    "        \"\"\"\n",
    "        エピソード開始時の処理(必要に応じてオーバーライド)\n",
    "        基底クラスにおいて config に基づき報酬計算対象の設定等が行われるため、\n",
    "        それ以外の追加処理や設定の上書きを行いたい場合のみオーバーライドする。\n",
    "        \"\"\"\n",
    "        super().onEpisodeBegin()\n",
    "\n",
    "\n",
    "    def onStepBegin(self):\n",
    "        \"\"\"\n",
    "        step 開始時の処理(必要に応じてオーバーライド)\n",
    "        基底クラスにおいて reward(step 報酬)を 0 にリセットしているため、\n",
    "        オーバーライドする場合、基底クラスの処理を呼び出すか、同等の処理が必要。\n",
    "        \"\"\"\n",
    "        super().onEpisodeBegin()\n",
    "    \n",
    "    \n",
    "    def onInnerStepBegin(self):\n",
    "        \"\"\"\n",
    "        インナーステップ開始時の処理(必要に応じてオーバーライド)\n",
    "        デフォルトでは何も行わないが、より細かい報酬計算が必要な場合に使用可能。\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def onInnerStepEnd(self):\n",
    "        \"\"\"\n",
    "        インナーステップ終了時の処理(必要に応じてオーバーライド)\n",
    "        一定周期で呼び出されるため、極力この関数で計算する方が望ましい。\n",
    "        \"\"\"\n",
    "        for team in self.reward: # team に属している Asset(Fighter)を取得する例\n",
    "            for f in self.manager.getAssets(lambda a:a.getTeam()==team and isinstance(a,Fighter)):\n",
    "                if(f().isAlive()):\n",
    "                    self.reward[team] += 0.1 #例えば、残存数に応じて報酬を与える場合\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 学習の実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したエージェントを学習する. 学習に使用する環境やモデルや学習条件などが`./sample_config.yml`として与えられている. 特に\"env_args\"の\"userModuleID\"と\"model_args\"にはそれぞれ実装したエージェント(`MyAgent.py`)が保存されるディレクトリ(デフォルトでは`Test`)とその引数ファイル(`args.json`)を設定する(`./sample_config.yml`参照). \n",
    "\n",
    "\n",
    "エージェントに対する設定ファイル(`agent_config.json`)において, 今回はobservationは数値ベクトル(`numpy.ndarray`)としているので以下の設定とする.\n",
    "\n",
    "- use_image_observation: false\n",
    "- use_vector_observation: true\n",
    "\n",
    "\n",
    "例えば学習条件としてエポック数を変えたい場合は`sample_config.yml`の`train_args`で`epochs`を変えればよい(-1に設定すると上限なしとなる.).\n",
    "\n",
    "```Yaml\n",
    "train_args:\n",
    "    epochs: 5\n",
    "```\n",
    "\n",
    "詳細は`docs/基準シミュレータ 取扱説明書.pdf`の6.7.4を参照すること.\n",
    "\n",
    "エージェント(`MyAgent.py`)とその引数ファイル(`args.json`), エージェントに対する設定ファイル(`agent_config.json`)を`./Test`以下に格納しておく. すると`./Test`は以下のようなディレクトリ構造になる.\n",
    "\n",
    "```bash\n",
    "Test\n",
    "├─ __init__.py\n",
    "├─ agent_config.json\n",
    "├─ args.json\n",
    "└─ MyAgent.py\n",
    "```\n",
    "\n",
    "そして以下のコマンドを実行する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/seminar\n",
    "!python main.py sample_config.yml --train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデルは`./results/Multi/YYYYmmddHHMMSS/policies/checkpoints`以下に保存される(pthファイル)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 投稿可能なプログラム一式としてまとめる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習済みモデル(例えば`Learner-latest.pth`)を`./Test`以下に格納する. `args.json`の\"weightPath\"の値を`Learner-latest.pth`としておく. また, 学習時に使用したモデルの設定ファイルと同等のものとして`model_config.yaml`を格納する(`sample_config.yml`の\"policy_config\"などの内容と整合することを確認しておく.). そして最終的に以下のようなディレクトリ構造となることを確認.\n",
    "\n",
    "```bash\n",
    "Test\n",
    "├─ __init__.py\n",
    "├─ agent_config.json\n",
    "├─ args.json\n",
    "├─ Learner-latest.pth\n",
    "├─ model_config.yaml\n",
    "└─ MyAgent.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 対戦を実行する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したエージェントを初期行動判断モデルと戦わせる. 事前に`/content/simulator_dist`直下に上記で作成したプログラム一式を配置しておく."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/simulator_dist\n",
    "!python validate.py --num-validation 1 --agent-module-path ./Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "適宜`--color`を\"Blue\"や\"Red\"に変えて陣営の種類に応じた行動が取れているかなどを確認する(実際の対戦では陣営の色はランダムに決まる). また, `--movie`と`--visualize`を`1`にして実際にどのように動いているか(google colaboratory上ではできない)を確認するなりログを参考にしてobservationの作成方法に工夫を施したり`sample_config.yml`で使用するモデルを変えたり学習の仕方に工夫を施すなどしてアルゴリズムをよりよくする."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 応募用ファイルを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作成したプログラムをzipファイルとして圧縮する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/simulator_dist\n",
    "!zip -r submit ./Test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
